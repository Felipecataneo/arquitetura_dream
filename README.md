
# ğŸ§  DREAM V13.2 â€” Sistema AGI Multi-LLM Robusto com API AssÃ­ncrona

---

## ğŸ“˜ DescriÃ§Ã£o

**DREAM (Dynamic Reasoning and Epistemic Metacognition) V13.2** Ã© a mais recente evoluÃ§Ã£o de uma arquitetura cognitiva que transforma LLMs em agentes de raciocÃ­nio robustos e autoconscientes. Esta versÃ£o representa um salto em capacidade e escalabilidade, introduzindo:

1.  **EstratÃ©gias de RaciocÃ­nio AvanÃ§adas:** O sistema agora vai alÃ©m da simples resposta, empregando tÃ©cnicas como **debate adversarial** para verificaÃ§Ã£o de fatos, **autocrÃ­tica e refinamento** para questÃµes complexas, e **decomposiÃ§Ã£o hierÃ¡rquica** para sÃ­nteses amplas.
2.  **GeraÃ§Ã£o de CÃ³digo de NÃ­vel Profissional:** O handler de cÃ³digo foi completamente redesenhado para analisar requisitos, planejar arquiteturas, gerar cÃ³digo de alta qualidade com templates de fallback e atÃ© mesmo tentar uma execuÃ§Ã£o segura.
3.  **API AssÃ­ncrona com FastAPI:** O backend foi migrado para FastAPI, permitindo o processamento de tarefas complexas e demoradas em segundo plano. Os usuÃ¡rios recebem um `task_id` imediatamente e podem consultar o status e o resultado posteriormente, evitando timeouts e melhorando a experiÃªncia do usuÃ¡rio.

O nÃºcleo do sistema continua a combater a "IlusÃ£o de Pensamento" â€” respostas convincentes, mas falhas â€” utilizando uma camada metacognitiva de controle, validaÃ§Ã£o e a filosofia de **humildade epistÃªmica**: saber quando nÃ£o sabe e aplicar a melhor estratÃ©gia para cada desafio.

---

## ğŸ“ Estrutura do Projeto

```bash
ğŸ“¦ dream-v13.2
â”œâ”€â”€ arquiteto_final.py        # NÃºcleo cognitivo V13.2 com pipeline, estratÃ©gias avanÃ§adas e geraÃ§Ã£o de cÃ³digo robusta
â”œâ”€â”€ server.py                 # Backend FastAPI para servir o sistema via API assÃ­ncrona
â”œâ”€â”€ frontend.html             # Interface web que interage com a API assÃ­ncrona (com polling)
â”œâ”€â”€ requirements.txt          # DependÃªncias Python atualizadas
â”œâ”€â”€ .env                      # Arquivo para chaves de API (NÃƒO ENVIAR PARA O GIT)
â””â”€â”€ README.md                 # Este arquivo
```

---

## ğŸš€ Como Rodar Localmente

### 1. PrÃ©-requisitos

- **Git**: Para clonar o repositÃ³rio.
- **Python 3.8+**: Para executar o backend.
- **Ollama**: (Opcional, para usar modelos locais) Certifique-se de que o [Ollama](https://ollama.com) estÃ¡ instalado e em execuÃ§Ã£o.
- **Chave da API OpenAI**: (Opcional, para usar GPT) Obtenha uma chave em [platform.openai.com](https://platform.openai.com/).

### 2. Clone o repositÃ³rio

```bash
git clone https://github.com/felipecataneo/arquitetura_dream.git
cd arquitetura_dream
```

### 3. Configure as DependÃªncias e Chaves

> Ã‰ altamente recomendado usar um ambiente virtual:

```bash
python -m venv venv
source venv/bin/activate  # No Windows: venv\Scripts\activate
```

**a. Instale as dependÃªncias:**

```bash
pip install -r requirements.txt
```

**`requirements.txt` atualizado:**
```txt
fastapi
uvicorn[standard]
python-dotenv
ollama
openai
```

**b. Crie o arquivo de ambiente:**

Crie um arquivo chamado `.env` na raiz do projeto e adicione sua chave da API da OpenAI (se for usar):

```
OPENAI_API_KEY="sk-sua-chave-secreta-da-openai-aqui"
```
> **Importante:** Adicione `.env` ao seu arquivo `.gitignore` para nunca enviÃ¡-lo para o repositÃ³rio.

### 4. Inicie os Modelos Locais (Opcional)

Se quiser usar modelos locais, inicie o Ollama e baixe o modelo desejado (o padrÃ£o Ã© `gemma3`):

```bash
ollama run gemma3
```

> O sistema funcionarÃ¡ mesmo que o Ollama nÃ£o esteja rodando, desde que a chave da OpenAI esteja configurada.

### 5. Execute o Servidor FastAPI

Na raiz do projeto, execute o servidor com **Uvicorn**:

```bash
uvicorn server:app --reload
```

A API estarÃ¡ disponÃ­vel na porta `8000` com os seguintes endpoints principais:
```
- http://127.0.0.1:8000/solve_async (POST) -> Inicia uma tarefa e retorna um task_id.
- http://127.0.0.1:8000/task/{task_id} (GET) -> Verifica o status de uma tarefa.
- http://127.0.0.1:8000/available_models (GET) -> Lista os modelos disponÃ­veis.
- http://127.0.0.1:8000/solve (POST) -> Endpoint sÃ­ncrono para compatibilidade.
```

### 6. Abra a Interface Web

Abra o arquivo `frontend.html` no seu navegador. A interface irÃ¡ se conectar Ã  API rodando na porta `8000`, carregar dinamicamente os modelos disponÃ­veis e interagir de forma assÃ­ncrona com o backend.

---

## ğŸ’¡ Exemplos de Perguntas

Use os botÃµes ou digite comandos na interface, selecionando o modelo desejado:

- ğŸ’» **CÃ³digo:** `FaÃ§a uma animaÃ§Ã£o web com um pentÃ¡gono rotativo e uma bola com fÃ­sica dentro dele`
- ğŸ§© **LÃ³gica:** `Um grupo de 4 pessoas com tempos de 1, 2, 5 e 10 minutos precisa cruzar uma ponte com uma tocha. Qual o tempo mÃ­nimo?`
- ğŸ”¬ **AcadÃªmico:** `Explique a teoria da relatividade geral usando o mÃ©todo de debate adversarial para garantir a precisÃ£o.`
- ğŸ“œ **SÃ­ntese:** `FaÃ§a uma sÃ­ntese completa sobre a histÃ³ria da inteligÃªncia artificial, decompondo o problema em sub-questÃµes.`

---

## ğŸ§  Principais Recursos da V13.2

- **âœ¨ Suporte Multi-Modelo:** Alterne facilmente entre LLMs locais (`gemma3`) e de nuvem (`gpt-4o`, `gpt-4o-mini`) atravÃ©s da interface.
- **ğŸš€ API AssÃ­ncrona com FastAPI:** Arquitetura robusta e escalÃ¡vel que processa tarefas complexas em segundo plano, melhorando drasticamente a experiÃªncia do usuÃ¡rio.
- **ğŸ› ï¸ GeraÃ§Ã£o de CÃ³digo Robusta:** Pipeline completo que inclui anÃ¡lise de requisitos, planejamento de arquitetura, geraÃ§Ã£o de cÃ³digo de alta qualidade, validaÃ§Ã£o e templates de fallback.
- **âš”ï¸ EstratÃ©gias de RaciocÃ­nio AvanÃ§adas:**
    - **Debate Adversarial:** Usa um modelo "proponente" e um "desafiador" para verificar fatos e aumentar a precisÃ£o.
    - **AutocrÃ­tica e Refinamento:** Gera um rascunho, critica-o rigorosamente e depois o refina.
    - **DecomposiÃ§Ã£o HierÃ¡rquica:** Quebra problemas amplos em sub-questÃµes gerenciÃ¡veis e depois sintetiza a resposta final.
- **ğŸ“š Humildade EpistÃªmica:** O sistema continua a reconhecer os limites do seu prÃ³prio conhecimento, aplicando a melhor estratÃ©gia para cada desafio.
- **ğŸ“ˆ Sistema de Cache, SaÃºde e MÃ©tricas:** Otimiza o desempenho, evita trabalho redundante e monitora a estabilidade do sistema em tempo real.

---

## ğŸ› ï¸ Tecnologias Usadas

- Python 3.8+
- **FastAPI** + **Uvicorn** (API REST assÃ­ncrona de alta performance)
- [Ollama](https://ollama.com) (para executar LLMs locais)
- [OpenAI API](https://platform.openai.com/) (para LLMs de nuvem)
- `python-dotenv` (para gerenciamento de chaves de API)
- HTML/CSS/JS puro (interface interativa com polling para resultados assÃ­ncronos)
- ProgramaÃ§Ã£o Orientada a Objetos com `abc` para abstraÃ§Ã£o de modelos.

---

## ğŸ“œ LicenÃ§a

Este projeto Ã© distribuÃ­do sob a LicenÃ§a MIT.  
Â© 2024 â€” Felipe Cataneo