
# ğŸ§  DREAM V12.3 â€” Sistema Cognitivo Multi-Modelo com Humildade EpistÃªmica

---

## ğŸ“˜ DescriÃ§Ã£o

**DREAM (Dynamic Reasoning and Epistemic Metacognition)** Ã© uma arquitetura cognitiva evolutiva que transforma LLMs em agentes autoconscientes de suas limitaÃ§Ãµes. Na versÃ£o 12.3, o sistema se torna **multi-modelo**, permitindo a alternÃ¢ncia dinÃ¢mica entre LLMs locais (via Ollama) e de nuvem (via OpenAI), combinando o melhor dos dois mundos: privacidade e velocidade com poder e conhecimento de ponta.

O nÃºcleo do sistema combate a "IlusÃ£o de Pensamento" â€” respostas convincentes, mas logicamente falhas â€” utilizando uma camada metacognitiva de controle, validaÃ§Ã£o simbÃ³lica e uma filosofia de **humildade epistÃªmica**: saber quando **nÃ£o sabe**.

---

## ğŸ“ Estrutura do Projeto

```bash
ğŸ“¦ dream-v12.3
â”œâ”€â”€ arquiteto_final.py        # NÃºcleo cognitivo com pipeline multi-modelo
â”œâ”€â”€ server.py                 # Backend Flask para servir o sistema via API
â”œâ”€â”€ frontend.html             # Interface web com seletor de modelo
â”œâ”€â”€ requirements.txt          # DependÃªncias Python
â”œâ”€â”€ .env                      # Arquivo para chaves de API (NÃƒO ENVIAR PARA O GIT)
â””â”€â”€ README.md                 # Este arquivo
```

---

## ğŸš€ Como Rodar Localmente

### 1. PrÃ©-requisitos

- **Git**: Para clonar o repositÃ³rio.
- **Python 3.8+**: Para executar o backend.
- **Ollama**: (Opcional, para usar modelos locais) Certifique-se de que o [Ollama](https://ollama.com) estÃ¡ instalado.
- **Chave da API OpenAI**: (Opcional, para usar GPT) Obtenha uma chave em [platform.openai.com](https://platform.openai.com/).

### 2. Clone o repositÃ³rio

```bash
git clone https://github.com/felipecataneo/arquitetura_dream.git
cd arquitetura_dream
```

### 3. Configure as DependÃªncias e Chaves

> Recomendado usar um ambiente virtual:

```bash
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate
```

**a. Instale as dependÃªncias:**

```bash
pip install -r requirements.txt
```

**`requirements.txt` atualizado:**
```txt
ollama
numpy
networkx
Flask 
flask-cors 
openai
dotenv
```

**b. Crie o arquivo de ambiente:**

Crie um arquivo chamado `.env` na raiz do projeto e adicione sua chave da API da OpenAI (se for usar):

```
OPENAI_API_KEY="sk-sua-chave-secreta-da-openai-aqui"
```
> **Importante:** Adicione `.env` ao seu arquivo `.gitignore` para nunca enviÃ¡-lo para o repositÃ³rio.

### 4. Inicie os Modelos

**a. Modelo Local (Ollama):**

Se quiser usar modelos locais, inicie o Ollama e baixe o modelo desejado (o padrÃ£o Ã© `gemma3`):

```bash
ollama run gemma3
```

> O sistema funcionarÃ¡ mesmo que o Ollama nÃ£o esteja rodando, desde que a chave da OpenAI esteja configurada.

**b. Modelo de Nuvem (OpenAI):**

Nenhuma aÃ§Ã£o Ã© necessÃ¡ria, desde que a chave no arquivo `.env` esteja correta.

### 5. Execute o Servidor

Na raiz do projeto, execute:

```bash
python server.py
```

A API estarÃ¡ disponÃ­vel com dois endpoints:
```
- http://127.0.0.1:5000/solve (POST)
- http://127.0.0.1:5000/available_models (GET)
```

### 6. Abra a Interface Web

Abra o arquivo `frontend.html` no seu navegador. A interface irÃ¡ carregar dinamicamente os modelos que estiverem disponÃ­veis (Ollama e/ou OpenAI) em um menu suspenso.

---

## ğŸ’¡ Exemplos de Comando RÃ¡pido

Use os botÃµes ou digite comandos na interface, selecionando o modelo desejado:

- ğŸ’» `Crie uma calculadora Python simples com GUI usando tkinter`
- ğŸª `O que pesa mais, um quilograma de aÃ§o ou um quilograma de penas?`
- ğŸ”¢ `Quantas vezes a letra e aparece nesta frase?`
- ğŸ”¬ `Explique a relaÃ§Ã£o entre relatividade geral e mecÃ¢nica quÃ¢ntica.`

---

## ğŸ§  Principais Recursos da V12.3

- **âœ¨ Suporte Multi-Modelo:** Alterne facilmente entre LLMs locais (`gemma3`) e de nuvem (`gpt-4o-mini`) atravÃ©s da interface.
- âœ… **ClassificaÃ§Ã£o de IntenÃ§Ã£o:** Identifica se o pedido Ã© para cÃ³digo, lÃ³gica, fatos, etc.
- ğŸ§© **Executor SimbÃ³lico:** Valida a lÃ³gica de problemas passo a passo.
- ğŸ”„ **Feedback Loop com Auto-correÃ§Ã£o:** Utiliza fallback algorÃ­tmico quando a geraÃ§Ã£o neural falha.
- ğŸ“š **Humildade EpistÃªmica:** Reconhece e sinaliza os limites do seu prÃ³prio conhecimento.
- ğŸ’¡ **EstratÃ©gias Especializadas:** Aplica o mÃ©todo de raciocÃ­nio mais adequado para cada tipo de pergunta.
- ğŸ“ˆ **Sistema de Cache, SaÃºde e MÃ©tricas:** Otimiza o desempenho e monitora a estabilidade.

---

## ğŸ› ï¸ Tecnologias Usadas

- Python 3.8+
- [Ollama](https://ollama.com) (para executar LLMs locais)
- [OpenAI API](https://platform.openai.com/) (para LLMs de nuvem)
- Flask + Flask-CORS (API REST)
- python-dotenv (para gerenciamento de chaves de API)
- HTML/CSS/JS puro (interface interativa e responsiva)
- ProgramaÃ§Ã£o Orientada a Objetos com `abc` para abstraÃ§Ã£o de modelos.

---

## ğŸ§ª Testes de Robustez

A arquitetura DREAM passou por uma evoluÃ§Ã£o com vÃ¡rias fases, incluindo:

1.  Agente neuro-simbÃ³lico ingÃªnuo
2.  Executor com correÃ§Ã£o simbÃ³lica
3.  Planejador generalista com classificaÃ§Ã£o de domÃ­nio
4.  Virada epistÃªmica: admitir a ignorÃ¢ncia e sugerir pesquisa
5.  **RefatoraÃ§Ã£o Multi-Modelo: abstraÃ§Ã£o para suportar diferentes provedores de LLM**

---

## ğŸ§‘â€ğŸ’» Contribuindo

Fique Ã  vontade para abrir *Issues*, *Pull Requests*, ou sugerir melhorias.

Se quiser propor novas estratÃ©gias de raciocÃ­nio, handlers ou integraÃ§Ãµes com outros provedores de LLM, envie seu mÃ³dulo e documentaÃ§Ã£o.

---

## ğŸ“œ LicenÃ§a

Este projeto Ã© distribuÃ­do sob a LicenÃ§a MIT.  
Â© 2025 â€” Felipe Cataneo